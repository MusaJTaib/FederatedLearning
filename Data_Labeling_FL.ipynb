{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482cfbf3-41ea-415d-9d68-d3e0ed6d7845",
   "metadata": {},
   "source": [
    "# CHF Shelter Data Federated Learning Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e373d99-6a3e-45d1-af63-e3769bd9c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9913a",
   "metadata": {},
   "source": [
    "Agencies: 4, 13, 55, 188, 213, 225, 330, 333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDays = 548\n",
    "Data_Days = 90\n",
    "Data_periods = 10\n",
    "Agency = 333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ae42a-aef1-4d88-aaf7-eff0e10073b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5049255-5384-483b-8965-3cfb84bde972",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = pd.read_parquet('MergedShelterData-Nov17.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6bd16-a4b9-4565-9976-fd6146196310",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b8253-ce4d-4288-bb18-db5361fca9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming tbl is your DataFrame\n",
    "tbl['Date'] = pd.to_datetime(tbl['Date'])\n",
    "\n",
    "def filter_group(group):\n",
    "    group = group.sort_values('Date')\n",
    "    min_date = group['Date'].min()\n",
    "    # Keep only dates within the first 90 days\n",
    "    group = group[group['Date'] <= min_date + pd.Timedelta(days=LDays)]\n",
    "    # Drop duplicates in the Date column\n",
    "    group = group.drop_duplicates(subset='Date', keep=False)\n",
    "    return group\n",
    "\n",
    "tbl = tbl.groupby('ClientId').apply(filter_group).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa661bc8-acae-4288-ab7f-936a399daa67",
   "metadata": {},
   "source": [
    "## EDA\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802cffce-b139-404d-96d6-a4f72a9355c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dates: {tbl.Date.min()} to {tbl.Date.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0403965-e6c5-4e78-a47a-ad52328fcef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nPrsn = len(tbl.ClientId.unique())\n",
    "print(f'{nPrsn} people in the data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4cea9-45ad-4780-a208-0b63f76eae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(tbl.Agency.unique())} different shelters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c746d0c-aadc-411a-a4d2-58e750b28052",
   "metadata": {},
   "source": [
    "#### Number of people who use different shelters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4740b8-2b63-4cd8-95d6-77e54e97fe3e",
   "metadata": {},
   "source": [
    "Total Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59a35c-3986-4caa-9d8c-e107423c3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "nShelter = tbl.groupby('ClientId').progress_apply(lambda x: len(x.Agency.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488e32ed-6861-4d10-a912-623fac205b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_shelter_breakdown(nShelter,nPrsn):\n",
    "    hist = nShelter.value_counts()\n",
    "    for nS in hist.index.sort_values():\n",
    "        print(f'{hist[nS]}/{nPrsn} ({100*hist[nS]/nPrsn:.2f}%) people used {nS} shelters.')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e2b4e-d37c-4b2b-ae63-1d7f3367c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_shelter_breakdown(nShelter,nPrsn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218bcf2f-4d1e-430b-8c30-8b1281ef2113",
   "metadata": {},
   "source": [
    "Heavy System Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590ee38-2e7f-400d-a30a-2de2a3198783",
   "metadata": {},
   "outputs": [],
   "source": [
    "nStay = tbl.groupby('ClientId').Date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f67a0-18a9-4c14-849e-f57f37ca765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heavyPctl = 0.95\n",
    "heavyIds = nStay.sort_values().iloc[int(nPrsn*heavyPctl):].index.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f555f4ff-98d8-4723-b18d-5fe12d2432be",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_shelter_breakdown(nShelter[heavyIds],len(heavyIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51b28a-1755-4c85-88d5-c11593a6c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_dfs = {}\n",
    "unique_agencies = tbl['Agency'].unique()\n",
    "\n",
    "for agency in unique_agencies:\n",
    "    agency_dfs[agency] = tbl[tbl['Agency'] == agency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_agency = agency_dfs[Agency] \n",
    "df_agency_4 = agency_dfs[4]  \n",
    "df_agency_13 = agency_dfs[13]  \n",
    "df_agency_55 = agency_dfs[55]  \n",
    "df_agency_188 = agency_dfs[188]  \n",
    "df_agency_213 = agency_dfs[213]  \n",
    "df_agency_225 = agency_dfs[225]  \n",
    "df_agency_330 = agency_dfs[330]  \n",
    "df_agency_333 = agency_dfs[333] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodeGap = 30 # days\n",
    "\n",
    "def calc_stays_and_gaps(tbl):    \n",
    "    stayDates = tbl.Date.drop_duplicates().sort_values() \n",
    "    nStay = len(stayDates)\n",
    "\n",
    "    gapVals = stayDates.diff()\n",
    "    nEpi = len(gapVals.loc[gapVals >= pd.Timedelta(f'{episodeGap} day') ])+1\n",
    "    \n",
    "    return pd.Series({ 'NStays': nStay, 'NEpisodes': nEpi })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c601ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agency_4G = df_agency_4.groupby('ClientId').progress_apply(calc_stays_and_gaps)\n",
    "df_agency_13G = df_agency_13.groupby('ClientId').progress_apply(calc_stays_and_gaps)\n",
    "df_agency_55G = df_agency_55.groupby('ClientId').progress_apply(calc_stays_and_gaps)\n",
    "df_agency_188G = df_agency_188.groupby('ClientId').progress_apply(calc_stays_and_gaps)\n",
    "df_agency_213G = df_agency_213.groupby('ClientId').progress_apply(calc_stays_and_gaps)\n",
    "df_agency_225G = df_agency_225.groupby('ClientId').progress_apply(calc_stays_and_gaps)\n",
    "df_agency_330G = df_agency_330.groupby('ClientId').progress_apply(calc_stays_and_gaps)\n",
    "df_agency_333G = df_agency_333.groupby('ClientId').progress_apply(calc_stays_and_gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5198d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agency_4G.reset_index(inplace=True)\n",
    "df_agency_13G.reset_index(inplace=True)\n",
    "df_agency_55G.reset_index(inplace=True)\n",
    "df_agency_188G.reset_index(inplace=True)\n",
    "df_agency_213G.reset_index(inplace=True)\n",
    "df_agency_225G.reset_index(inplace=True)\n",
    "df_agency_330G.reset_index(inplace=True)\n",
    "df_agency_333G.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a3e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agency_4G['ClientId'] = df_agency_4G['ClientId'].astype(str) + '_4'\n",
    "df_agency_4G = df_agency_4G.set_index('ClientId')\n",
    "\n",
    "df_agency_13G['ClientId'] = df_agency_13G['ClientId'].astype(str) + '_13'\n",
    "df_agency_13G = df_agency_13G.set_index('ClientId')\n",
    "\n",
    "df_agency_55G['ClientId'] = df_agency_55G['ClientId'].astype(str) + '_55'\n",
    "df_agency_55G = df_agency_55G.set_index('ClientId')\n",
    "\n",
    "df_agency_188G['ClientId'] = df_agency_188G['ClientId'].astype(str) + '_188'\n",
    "df_agency_188G = df_agency_188G.set_index('ClientId')\n",
    "\n",
    "df_agency_213G['ClientId'] = df_agency_213G['ClientId'].astype(str) + '_213'\n",
    "df_agency_213G = df_agency_213G.set_index('ClientId')\n",
    "\n",
    "df_agency_225G['ClientId'] = df_agency_225G['ClientId'].astype(str) + '_225'\n",
    "df_agency_225G = df_agency_225G.set_index('ClientId') \n",
    "\n",
    "df_agency_330G['ClientId'] = df_agency_330G['ClientId'].astype(str) + '_330'\n",
    "df_agency_330G = df_agency_330G.set_index('ClientId')\n",
    "\n",
    "df_agency_333G['ClientId'] = df_agency_333G['ClientId'].astype(str) + '_333'\n",
    "df_agency_333G = df_agency_333G.set_index('ClientId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def gen_cluster_labels(tbl):\n",
    "    dat = tbl.to_numpy()\n",
    "    nrm = (dat - dat.mean(axis=0))/np.sqrt(dat.var(axis=0))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=3, random_state=0).fit(nrm)\n",
    "    labels = kmeans.labels_    \n",
    "\n",
    "    labelVal = {}\n",
    "    labelVal['Trn'] = np.argmin(kmeans.cluster_centers_.sum(axis=1)) # Transitional: Fewest stays and episodes.\n",
    "    labelVal['Epi'] = np.argmax(kmeans.cluster_centers_[:,1])  # Chronic: Most stays.\n",
    "    labelVal['Chr'] = np.argmax(kmeans.cluster_centers_[:,0])  # Episodic: Most episodes.\n",
    "\n",
    "    print(\"Sum of centroids:\", kmeans.cluster_centers_.sum(axis=1))\n",
    "    print(\"Max in second feature:\", kmeans.cluster_centers_[:, 1])\n",
    "    print(\"Max in first feature:\", kmeans.cluster_centers_[:, 0])\n",
    "\n",
    "    cohort = {}\n",
    "    for k in labelVal.keys():\n",
    "        cohort[k] = tbl.loc[labels == labelVal[k]].index.to_numpy()\n",
    "\n",
    "    return cohort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83fbb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def predict_global_model(tbl, centroids):\n",
    "    # Normalize the data\n",
    "    dat = tbl.to_numpy()\n",
    "    nrm = (dat - dat.mean(axis=0)) / np.sqrt(dat.var(axis=0))\n",
    "\n",
    "    # Check if the number of features matches\n",
    "    if nrm.shape[1] != centroids.shape[1]:\n",
    "        raise ValueError(\"Number of features in the data does not match the number of features in the centroids.\")\n",
    "\n",
    "    # Calculate distances and get labels\n",
    "    distances = cdist(nrm, centroids, 'euclidean')\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "\n",
    "    # Determine the labels for each type\n",
    "    labelVal = {}\n",
    "    labelVal['Trn'] = np.argmin(centroids.sum(axis=1))  # Fewest stays and episodes\n",
    "    labelVal['Epi'] = np.argmax(centroids[:, 1])  # Most stays\n",
    "    labelVal['Chr'] = np.argmax(centroids[:, 0])  # Most episodes\n",
    "\n",
    "    # print(\"Sum of centroids:\", centroids.sum(axis=1))\n",
    "    # print(\"Max in second feature:\", centroids[:, 1])\n",
    "    # print(\"Max in first feature:\", centroids[:, 0])\n",
    "    # Assign records to cohorts based on labels\n",
    "    cohort = {}\n",
    "    for k, val in labelVal.items():\n",
    "        cohort[k] = tbl.iloc[labels == val].index.to_numpy()\n",
    "\n",
    "    return cohort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe648fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [\n",
    "    df_agency_4G,\n",
    "    df_agency_13G,\n",
    "    df_agency_55G,\n",
    "    df_agency_188G,\n",
    "    df_agency_213G,\n",
    "    df_agency_225G, \n",
    "    df_agency_330G,\n",
    "    df_agency_333G\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d2fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = []  # List to store centroids from each model\n",
    "for df in dataframes:  # Assuming 'dataframes' is a list of your 8 DataFrames\n",
    "    dat = df.to_numpy()\n",
    "    nrm = (dat - dat.mean(axis=0))/np.sqrt(dat.var(axis=0))\n",
    "    kmeans = KMeans(n_clusters=3, random_state=0).fit(nrm)\n",
    "    centroids.append(kmeans.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [len(df) for df in dataframes]\n",
    "total_weight = sum(weights)\n",
    "normalized_weights = [w / total_weight for w in weights]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf39f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_centroids = sum(w * c for w, c in zip(normalized_weights, centroids)) / len(dataframes)\n",
    "average_centroids = sum(w * c for w, c in zip(normalized_weights, centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31341fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_labels_4 = predict_global_model(df_agency_4G, average_centroids)\n",
    "global_labels_13 = predict_global_model(df_agency_13G, average_centroids)\n",
    "global_labels_55 = predict_global_model(df_agency_55G, average_centroids)\n",
    "global_labels_188 = predict_global_model(df_agency_188G, average_centroids)\n",
    "global_labels_213 = predict_global_model(df_agency_213G, average_centroids)\n",
    "global_labels_225 = predict_global_model(df_agency_225G, average_centroids)\n",
    "global_labels_330 = predict_global_model(df_agency_330G, average_centroids)\n",
    "global_labels_333 = predict_global_model(df_agency_333G, average_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_labels = {'Trn': [], 'Epi': [], 'Chr': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = [global_labels_4, global_labels_13, global_labels_55, global_labels_188, global_labels_213, global_labels_225, global_labels_330, global_labels_333]\n",
    "\n",
    "for d in dict_list:\n",
    "    for key in combined_labels.keys():\n",
    "        combined_labels[key].extend(d[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = sum(len(lst) for lst in combined_labels.values())\n",
    "print(\"Total number of elements in all lists:\", total_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dictionary into a list of tuples (label, value)\n",
    "data = []\n",
    "for label, values in combined_labels.items():\n",
    "    for value in values:\n",
    "        data.append((label, value))\n",
    "\n",
    "# Convert the list of tuples into a DataFrame\n",
    "combined_df = pd.DataFrame(data, columns=['Label', 'Value'])\n",
    "\n",
    "print(\"Number of rows in the DataFrame:\", len(combined_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a557e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Labels_df as a CSV file\n",
    "file_name = f'CHF_Data_1/CHF_Labels_FL2_{LDays}.csv'\n",
    "combined_df.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
