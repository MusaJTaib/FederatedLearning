{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_periods = 10\n",
    "Data_Days = 90\n",
    "LDays = 548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"Centralized_FL\"\n",
    "windows = f'{Data_periods}W'\n",
    "label_days = f'{Data_Days}-Central'\n",
    "prediction_days = f'{LDays}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "CHF_data = pd.read_csv(f'CHF_Data_1/CHF_Unlinked_{Data_Days}D_{Data_periods}W.csv')\n",
    "CHF_Labels_2 = pd.read_csv(f'CHF_Data_1/CHF_Labels_FL2_{LDays}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHF_Labels = pd.DataFrame()\n",
    "CHF_Labels['ClientId'] = CHF_Labels_2['Value'] \n",
    "CHF_Labels['Label'] = CHF_Labels_2['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHF_data['ClientId'] = CHF_data['ClientId'].astype(str)\n",
    "CHF_Labels['ClientId'] = CHF_Labels['ClientId'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHF_data['Agency'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHF_unique = CHF_data.drop_duplicates(subset='ClientId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agency_Ids = CHF_unique.groupby('Agency')['ClientId'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agency_Ids = Agency_Ids.set_index('Agency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Agency' and join 'ClientId' values separated by commas\n",
    "agency_dict = Agency_Ids.groupby('Agency')['ClientId'].apply(lambda x: ','.join(x.astype(str))).to_dict()\n",
    "\n",
    "# Convert the string of ClientIds into a list for each Agency and remove brackets\n",
    "for agency in agency_dict:\n",
    "    # Strip the brackets and then split by comma\n",
    "    agency_dict[agency] = agency_dict[agency].strip('[]').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_data = CHF_data.drop('Agency', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHF_unique.reset_index(drop=True, inplace=True)\n",
    "pivoted_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agency = CHF_unique['Agency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_data['Agency'] = Agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = agency_dict[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pivoted_data[pivoted_data['ClientId'].isin(agency_dict[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and preprocess data\n",
    "data = pd.merge(pivoted_data, CHF_Labels, on='ClientId')\n",
    "data['Label'] = data['Label'].map({'Trn': 1, 'Epi': 2, 'Chr': 3})  # Replace class1, class2, class3 with actual class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = data\n",
    "y = data['Label'].values\n",
    "y_indices = y - 1  # Convert labels to 0, 1, and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agency in agency_dict:\n",
    "    # Strip whitespaces and convert each string in the list to an integer\n",
    "    agency_dict[agency] = [client_id.strip() for client_id in agency_dict[agency]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a combined column for stratification\n",
    "data['stratify_col'] = data['Agency'].astype(str) + \"_\" + y_indices.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform the train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, \n",
    "#     y_indices, \n",
    "#     test_size=0.2, \n",
    "#     random_state=42, \n",
    "#     stratify=data['stratify_col']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the classes with too few samples\n",
    "filtered_data = X[~X['stratify_col'].isin([])]\n",
    "\n",
    "# Split the filtered data\n",
    "y['Label'] = filtered_data['Label']\n",
    "y['ClientId'] = filtered_data['ClientId']\n",
    "\n",
    "X1 = filtered_data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X1, y, test_size=0.2, random_state=42, stratify=filtered_data['stratify_col']\n",
    ")\n",
    "\n",
    "# Manually add the excluded samples back to the train or test set\n",
    "excluded_data = X[X['Label'].isin([])]\n",
    "\n",
    "# You can decide how to add these back, for example, to X_train and y_train\n",
    "# Just as an example, adding to X_train and y_train\n",
    "X_train = pd.concat([X_train, excluded_data])\n",
    "y_train = pd.concat([y_train, excluded_data['Label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHF_Labels_3A1 = pd.read_csv(f'CHF_Data_1/CHF_Labels_{LDays}.csv')\n",
    "CHF_Labels_3A = pd.DataFrame()\n",
    "CHF_Labels_3A['ClientId'] = CHF_Labels_3A1['ClientId'] \n",
    "CHF_Labels_3A['Label'] = CHF_Labels_3A1['ListNumber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHF_Labels_3B1 = pd.read_csv(f'CHF_Data_1/CHF_Labels_FL2_{LDays}.csv')\n",
    "CHF_Labels_3B = pd.DataFrame()\n",
    "CHF_Labels_3B['ClientId'] = CHF_Labels_3B1['Value'] \n",
    "CHF_Labels_3B['Label'] = CHF_Labels_3B1['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHF_Labels_3A['ClientId'] = CHF_Labels_3A['ClientId'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split ClientId in CHF_Labels_3B to extract IntegerA\n",
    "CHF_Labels_3B['IntegerA'] = CHF_Labels_3B['ClientId'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(CHF_Labels_3B, CHF_Labels_3A.rename(columns={'Label': 'Label_3A'}), \n",
    "                     left_on='IntegerA', right_on='ClientId', how='left')\n",
    "\n",
    "# Step 4: Update Label in CHF_Labels_3B with Label from CHF_Labels_3A where there's a match\n",
    "CHF_Labels_3B['Label'] = merged_df['Label_3A']\n",
    "\n",
    "# Step 5: Drop the temporary columns\n",
    "CHF_Labels_3B.drop(['IntegerA'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHF_Labels_4 = pd.DataFrame()\n",
    "CHF_Labels_4 = CHF_Labels_3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHF_Labels_4['Label'] = CHF_Labels_4['Label'].map({'Trn': 0, 'Epi': 1, 'Chr': 2})  # Replace class1, class2, class3 with actual class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.reset_index(drop=True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the second integer from 'ClientID' and convert it to int\n",
    "y_test['Integer2'] = y_test['ClientId'].apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "# Initialize a dictionary to hold the subsets of y_test\n",
    "dfs = {}\n",
    "\n",
    "# Loop through each unique 'integer2' value to create separate DataFrames\n",
    "for value in y_test['Integer2'].unique():\n",
    "    dfs[value] = y_test[y_test['Integer2'] == value]\n",
    "\n",
    "\n",
    "# Example: Access the DataFrame where 'integer2' is 1 (replace with actual unique values)\n",
    "y_test_4 = dfs[4]\n",
    "y_test_13 = dfs[13]\n",
    "y_test_55 = dfs[55]\n",
    "y_test_188 = dfs[188]\n",
    "y_test_213 = dfs[213]\n",
    "y_test_225 = dfs[225]\n",
    "y_test_330 = dfs[330]\n",
    "y_test_333 = dfs[333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the second integer from 'ClientID' and convert it to int\n",
    "X_test['Integer2'] = X_test['ClientId'].apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "# Initialize a dictionary to hold the subsets of y_test\n",
    "dfs = {}\n",
    "\n",
    "# Loop through each unique 'integer2' value to create separate DataFrames\n",
    "for value in X_test['Integer2'].unique():\n",
    "    dfs[value] = X_test[X_test['Integer2'] == value]\n",
    "\n",
    "\n",
    "# Example: Access the DataFrame where 'integer2' is 1 (replace with actual unique values)\n",
    "X_test_4 = dfs[4]\n",
    "X_test_13 = dfs[13]\n",
    "X_test_55 = dfs[55]\n",
    "X_test_188 = dfs[188]\n",
    "X_test_213 = dfs[213]\n",
    "X_test_225 = dfs[225]\n",
    "X_test_330 = dfs[330]\n",
    "X_test_333 = dfs[333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming dfs_keys is a list of your specific keys for which you have X_test and y_test DataFrames stored in dictionaries\n",
    "dfs_keys = [4, 13, 55, 188, 213, 225, 330, 333]\n",
    "\n",
    "# Assuming X_test_dfs and y_test_dfs are dictionaries holding your respective DataFrames\n",
    "X_test_dfs = {key: globals()[f'X_test_{key}'] for key in dfs_keys}\n",
    "y_test_dfs = {key: globals()[f'y_test_{key}'] for key in dfs_keys}\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for key in dfs_keys:\n",
    "    # Sort by 'ClientId'\n",
    "    X_test_dfs[key] = X_test_dfs[key].sort_values(by='ClientId')\n",
    "    y_test_dfs[key] = y_test_dfs[key].sort_values(by='ClientId')\n",
    "    \n",
    "    # Drop 'ClientId' and 'Integer2' columns from X_test_dfs\n",
    "    X_test_dfs[key] = X_test_dfs[key].drop(columns=['Integer2'])\n",
    "    y_test_dfs[key] = y_test_dfs[key].drop(columns=['ClientId', 'Integer2'])\n",
    "\n",
    "    # Apply StandardScaler to X_test\n",
    "    # Fit and transform, then convert back to DataFrame to retain column names\n",
    "    #X_test_dfs[key] = pd.DataFrame(scaler.fit_transform(X_test_dfs[key]), columns=X_test_dfs[key].columns, index=X_test_dfs[key].index)\n",
    "\n",
    "    # Squeeze y_test_dfs[key] if it's a numpy array\n",
    "    # This step assumes your y_test data is in a format that can be squeezed (e.g., a numpy array)\n",
    "    # If y_test_dfs[key] is a DataFrame or Series, this step may need adjustment\n",
    "    if isinstance(y_test_dfs[key], np.ndarray):\n",
    "        y_test_dfs[key] = np.squeeze(y_test_dfs[key])\n",
    "    elif isinstance(y_test_dfs[key], pd.Series):\n",
    "        y_test_dfs[key] = y_test_dfs[key].to_numpy().squeeze()\n",
    "\n",
    "\n",
    "\n",
    "# Update the global namespace if necessary, or work directly with the dictionaries\n",
    "for key in dfs_keys:\n",
    "    globals()[f'X_test_{key}'] = X_test_dfs[key]\n",
    "    globals()[f'y_test_{key}'] = y_test_dfs[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reset_index(drop=True)\n",
    "X_test['Label'] = y_test['Label']\n",
    "X_test.dropna(subset=['Label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_counts = X['stratify_col'].value_counts()\n",
    "# print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to PyTorch tensors\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "# X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "# y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear((X_train.shape[1]-4), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),  # Dropout layer\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),  # Another Dropout layer\n",
    "            nn.Linear(128, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),  # Another Dropout layer\n",
    "            nn.Linear(16, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "#model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X  # Features (already processed)\n",
    "        self.y = y  # Labels (already processed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        sample = {\n",
    "            \"data\": self.X[idx],\n",
    "            \"label\": self.y[idx]\n",
    "        }\n",
    "        # Retrieve the data and label at the given index\n",
    "        # X_sample = self.X[idx]\n",
    "        # y_sample = self.y[idx]\n",
    "\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_tensor = torch.tensor(np.array([1, 12, 18]), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientUpdate(object):\n",
    "  def __init__(self, dataset, batchSize, learning_rate, epochs, idxs):\n",
    "    # Filter the DataFrame and split into X and y\n",
    "    # Removing extra quotes from each element in the list\n",
    "    idxs = [s.strip(\"'\") for s in idxs]\n",
    "    dataset['ClientId'] = dataset['ClientId'].astype(str)\n",
    "    y = dataset[dataset['ClientId'].isin(idxs)][['Label']]  # Assuming idxs is your client indices list\n",
    "    X = dataset[dataset['ClientId'].isin(idxs)].drop(['Label', 'ClientId', 'Agency','stratify_col'], axis=1)\n",
    "\n",
    "    # Convert to tensors (outside of CustomDataset)\n",
    "    X_tensor = torch.tensor(X.values.astype(float), dtype=torch.float32)\n",
    "    y_tensor = (torch.tensor(y.values, dtype=torch.long) - 1).squeeze()  # Subtract 1 for zero-based indexing\n",
    "    # Number of classes (assuming you know the number of classes in your task)\n",
    "    num_classes = 3  # Change this to the actual number of classes\n",
    "\n",
    "    # Perform one-hot encoding\n",
    "    y_one_hot = F.one_hot(y_tensor, num_classes=num_classes)\n",
    "    y_one_hot = y_one_hot.float()\n",
    "    #print(X_tensor.shape,y_one_hot)\n",
    "\n",
    "    # Create instance of CustomDataset\n",
    "    #custom_dataset = CustomDataset(X_tensor, y_tensor, idxs)\n",
    "    self.train_loader = DataLoader(CustomDataset(X_tensor, y_one_hot), batch_size=batchSize, shuffle=True)\n",
    "    #print(len(self.train_loader.dataset))\n",
    "    self.learning_rate = learning_rate\n",
    "    self.epochs = epochs\n",
    "\n",
    "  def train(self, model):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor) \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.5)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    e_loss = []\n",
    "    for epoch in range(1, self.epochs+1):\n",
    "\n",
    "      train_loss = 0.0\n",
    "\n",
    "      model.train()\n",
    "      #for data, labels in self.train_loader:\n",
    "      for batch in self.train_loader:\n",
    "        data = batch[\"data\"]\n",
    "        labels = batch[\"label\"]\n",
    "        #print(f\"Data in train loader {data.shape} and Labels in train loader {labels.shape}\")\n",
    "        if torch.cuda.is_available():\n",
    "          data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # make a forward pass\n",
    "        output = model(data)\n",
    "        #print(\"Output\",output.shape,output.type,output)\n",
    "        #print(\"Labels\",labels.shape,labels.type,labels)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, labels)\n",
    "        # do a backwards pass\n",
    "        loss.backward()\n",
    "        # perform a single optimization step\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "      # average losses\n",
    "      train_loss = train_loss/len(self.train_loader.dataset)\n",
    "      e_loss.append(train_loss)\n",
    "\n",
    "    total_loss = sum(e_loss)/len(e_loss)\n",
    "\n",
    "    return model.state_dict(), total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, rounds, batch_size, lr, ds, data_dict, C, K, E, plt_title, plt_color):\n",
    "  \"\"\"\n",
    "  Function implements the Federated Averaging Algorithm from the FedAvg paper.\n",
    "  Specifically, this function is used for the server side training and weight update\n",
    "\n",
    "  Params:\n",
    "    - model:           PyTorch model to train\n",
    "    - rounds:          Number of communication rounds for the client update\n",
    "    - batch_size:      Batch size for client update training\n",
    "    - lr:              Learning rate used for client update training\n",
    "    - ds:              Dataset used for training\n",
    "    - data_dict:       Type of data partition used for training (IID or non-IID)\n",
    "    - C:               Fraction of clients randomly chosen to perform computation on each round\n",
    "    - K:               Total number of clients\n",
    "    - E:               Number of training passes each client makes over its local dataset per round\n",
    "    - tb_writer_name:  Directory name to save the tensorboard logs\n",
    "  Returns:\n",
    "    - model:           Trained model on the server\n",
    "  \"\"\"\n",
    "\n",
    "  # global model weights\n",
    "  global_weights = model.state_dict()\n",
    "\n",
    "  # training loss\n",
    "  train_loss = []\n",
    "  \n",
    "  # measure time\n",
    "  start = time.time()\n",
    "\n",
    "  for curr_round in range(1, rounds+1):\n",
    "    w, local_loss = [], []\n",
    "\n",
    "    m = max(int(C*K), 1)\n",
    "    \n",
    "    S_t = [4,13,55,188,213,225,330,333]\n",
    "    for k in S_t:\n",
    "      local_update = ClientUpdate(dataset=ds, batchSize=batch_size, learning_rate=lr, epochs=E, idxs=data_dict[k])\n",
    "      weights, loss = local_update.train(model=copy.deepcopy(model))\n",
    "\n",
    "      w.append(copy.deepcopy(weights))\n",
    "      local_loss.append(copy.deepcopy(loss))\n",
    "\n",
    "    # updating the global weights\n",
    "    weights_avg = copy.deepcopy(w[0])\n",
    "    for k in weights_avg.keys():\n",
    "      for i in range(1, len(w)):\n",
    "        weights_avg[k] += w[i][k]\n",
    "\n",
    "      weights_avg[k] = torch.div(weights_avg[k], len(w))\n",
    "\n",
    "    global_weights = weights_avg\n",
    "\n",
    "    # move the updated weights to our model state dict\n",
    "    model.load_state_dict(global_weights)\n",
    "\n",
    "    # loss\n",
    "    loss_avg = sum(local_loss) / len(local_loss)\n",
    "    print('Round: {}... \\tAverage Loss: {}'.format(curr_round, round(loss_avg, 3)))\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "  end = time.time()\n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(train_loss)\n",
    "  ax.plot(x_axis, y_axis, 'tab:'+plt_color)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Train Loss',\n",
    "       title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'.jpg', format='jpg')\n",
    "  print(\"Training Done!\")\n",
    "  print(\"Total time taken to Train: {}\".format(end-start))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, dataset, bs, criterion, num_classes, classes):\n",
    "    test_loss = 0.0\n",
    "    true_positives = np.zeros(num_classes)\n",
    "    false_positives = np.zeros(num_classes)\n",
    "    false_negatives = np.zeros(num_classes)\n",
    "\n",
    "    y = dataset['Label']\n",
    "    X = dataset.drop(['Label', 'ClientId', 'Agency', 'stratify_col'], axis=1)\n",
    "\n",
    "    X_tensor = torch.tensor(X.values.astype(np.float32), dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y.values, dtype=torch.long) - 1\n",
    "\n",
    "    y_one_hot = F.one_hot(y_tensor, num_classes=num_classes).float()\n",
    "\n",
    "    test_loader = DataLoader(CustomDataset(X_tensor, y_one_hot), batch_size=bs)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            data = batch[\"data\"]\n",
    "            labels = batch[\"label\"]\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels.max(dim=1)[1])\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "\n",
    "            _, pred = torch.max(output, 1)\n",
    "            true_labels = labels.max(dim=1)[1]\n",
    "\n",
    "            for i in range(num_classes):\n",
    "                true_positives[i] += ((pred == i) & (true_labels == i)).cpu().sum().item()\n",
    "                false_positives[i] += ((pred == i) & (true_labels != i)).cpu().sum().item()\n",
    "                false_negatives[i] += ((pred != i) & (true_labels == i)).cpu().sum().item()\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    macro_avg_precision = np.nanmean(precision)  # Use nanmean to handle division by zero\n",
    "    macro_avg_recall = np.nanmean(recall)\n",
    "\n",
    "    return macro_avg_precision, macro_avg_recall, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training rounds\n",
    "rounds = 75\n",
    "# client fraction\n",
    "C = 1\n",
    "# number of clients\n",
    "K = 8\n",
    "# number of training passes on local dataset for each round\n",
    "E = 15\n",
    "# batch size\n",
    "batch_size = 500\n",
    "# learning Rate\n",
    "lr=0.0195\n",
    "# load model\n",
    "MLP_NN = MLP()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3  # Example number of classes\n",
    "classes_test = ['Class 0', 'Class 1', 'Class 2']  # Example class names\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor) \n",
    "Agency_no = 333\n",
    "\n",
    "# Initialize sums for macro averages and class-wise metrics\n",
    "sum_macro_avg_precision = 0\n",
    "sum_macro_avg_recall = 0\n",
    "sum_class_precision = np.zeros(num_classes)\n",
    "sum_class_recall = np.zeros(num_classes)\n",
    "runs = 5\n",
    "# Train and test the model 10 times\n",
    "for _ in range(runs):\n",
    "    MLP_NN_iid_trained = training(MLP_NN, rounds, batch_size, lr, X_train, agency_dict, C, K, E, \"CHF\", 'red')\n",
    "    \n",
    "    macro_avg_precision, macro_avg_recall, class_precision, class_recall = testing(\n",
    "        model=MLP_NN_iid_trained, \n",
    "        dataset=X_test_333, \n",
    "        bs=batch_size, \n",
    "        criterion=criterion, \n",
    "        num_classes=num_classes, \n",
    "        classes=classes_test\n",
    "    )\n",
    "    \n",
    "    sum_macro_avg_precision += macro_avg_precision\n",
    "    sum_macro_avg_recall += macro_avg_recall\n",
    "    sum_class_precision += class_precision\n",
    "    sum_class_recall += class_recall\n",
    "\n",
    "# Calculate the average of macro-averaged precision and recall over all runs\n",
    "average_macro_avg_precision = sum_macro_avg_precision / runs\n",
    "average_macro_avg_recall = sum_macro_avg_recall / runs\n",
    "average_class_precision = sum_class_precision / runs\n",
    "average_class_recall = sum_class_recall / runs\n",
    "\n",
    "print(f\"Average Macro Average Precision: {average_macro_avg_precision * 100:.2f}%\")\n",
    "print(f\"Average Macro Average Recall: {average_macro_avg_recall * 100:.2f}%\")\n",
    "for i, class_name in enumerate(classes_test):\n",
    "    print(f\"Average Precision for {class_name}: {average_class_precision[i] * 100:.2f}%\")\n",
    "    print(f\"Average Recall for {class_name}: {average_class_recall[i] * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these with your actual values\n",
    "# macro_avg_precision = precision # Replace with your actual value\n",
    "# macro_avg_recall = recall  # Replace with your actual value\n",
    "\n",
    "data = {\n",
    "    \"Model Type\": [model_type],\n",
    "    \"Runs\": [runs],\n",
    "    \"Agency\": [Agency_no],\n",
    "    \"Windows\": [windows],\n",
    "    \"Label Days\": [label_days],\n",
    "    \"Prediction Days\": [prediction_days],\n",
    "    \"Average Precision\": [macro_avg_precision],\n",
    "    \"Average Recall\": [macro_avg_recall],\n",
    "    \"Per class average precision\": [average_class_precision],\n",
    "    \"Per class average recall\": [average_class_recall]\n",
    "}\n",
    "\n",
    "new_data_df = pd.DataFrame(data)\n",
    "\n",
    "# File name\n",
    "excel_filename = \"NRes/Model_Results_FL.xlsx\"\n",
    "\n",
    "# Check if the file exists\n",
    "try:\n",
    "    # If it exists, read the existing data and append the new data\n",
    "    existing_data_df = pd.read_excel(excel_filename)\n",
    "    combined_df = pd.concat([existing_data_df, new_data_df], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, just use the new data\n",
    "    combined_df = new_data_df\n",
    "\n",
    "# Save the combined data back to the Excel file\n",
    "combined_df.to_excel(excel_filename, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
